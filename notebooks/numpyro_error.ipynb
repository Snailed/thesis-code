{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as nn\n",
    "from jax.nn import logsumexp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT = [\n",
    "    \"prec\",\n",
    "    \"w0\",\n",
    "    \"b0\",\n",
    "    \"w1\",\n",
    "    \"b1\",\n",
    "    \"w2\",\n",
    "    \"b2\",\n",
    "]\n",
    "OUT = [\"y_loc\", \"y\"]\n",
    "\n",
    "\n",
    "def BNN(X, y=None, depth=1, width=4, sigma=1.0, D_Y=None, activation=jnp.tanh, subsample=None):\n",
    "    # Make sure D_Y is defined\n",
    "    if y is None and D_Y is None:\n",
    "        raise ValueError(\"Either y or D_Y must be provided.\")\n",
    "    if y is not None:\n",
    "        if y.ndim > 1:\n",
    "            y = y.flatten()\n",
    "            D_Y = y.shape[-1]\n",
    "        else:\n",
    "            D_Y = 1\n",
    "\n",
    "    N = X.shape[-2]\n",
    "    D_X = X.shape[-1]\n",
    "    D_Z = width\n",
    "    if depth == 1:\n",
    "        D_Z = D_Y\n",
    "\n",
    "    # First layer\n",
    "    # OLD version (no index)\n",
    "    # w = numpyro.sample(\"w0\", dist.Normal(0.0, 1).expand((D_X, D_Z)))\n",
    "    # b = numpyro.sample(\"b0\", dist.Normal(0.0, 1).expand((D_Z, )))\n",
    "\n",
    "    # OR: Added variable index to allow subsampling\n",
    "    w0 = numpyro.sample(\"w0\", dist.Normal(0.0, 1).expand((D_X, D_Z)))\n",
    "    b0 = numpyro.sample(\"b0\", dist.Normal(0.0, 1).expand((D_Z, )))\n",
    "\n",
    "    # OLD version (no subsampling)\n",
    "    # z = X @ w + b.flatten()\n",
    "    # z_p = activation(z)\n",
    "\n",
    "    # Middle layers:\n",
    "    # OLD version\n",
    "    # for i in range(1, depth):\n",
    "    #     w = numpyro.sample(f\"w{i}\", dist.Normal(0.0, 1).expand((D_Z, D_Z)))\n",
    "    #     b = numpyro.sample(f\"b{i}\", dist.Normal(0.0, 1).expand((D_Z,)))\n",
    "    #     z = z_p @ w + b\n",
    "    #     z_p = activation(z)\n",
    "\n",
    "    # OR: added variable index to allow subsampling\n",
    "    # OR: made sample site index  explicit(got None type error when tracing in [predictive])\n",
    "    # OR: did not investigate the problem in detail by probably i gets tra\n",
    "    w1 = numpyro.sample(f\"w{1}\", dist.Normal(0.0, 1).expand((D_Z, D_Z)))\n",
    "    b1 = numpyro.sample(f\"b{1}\", dist.Normal(0.0, 1).expand((D_Z,)))\n",
    "    # OLD version (no subsampling)\n",
    "    # z = z_p @ w + b\n",
    "    # z_p = activation(z)\n",
    "\n",
    "    # Last layer\n",
    "    # OLD version (no variable index)\n",
    "    # w = numpyro.sample(f\"w{2}\", dist.Normal(0.0, 1).expand((D_Z, D_Y)))\n",
    "    # b = numpyro.sample(f\"b{2}\", dist.Normal(0.0, 1).expand((D_Y,)))\n",
    "\n",
    "    # OR: added variable index to allow subsampling\n",
    "    w2 = numpyro.sample(f\"w{2}\", dist.Normal(0.0, 1).expand((D_Z, D_Y)))\n",
    "    b2 = numpyro.sample(f\"b{2}\", dist.Normal(0.0, 1).expand((D_Y,)))\n",
    "    # OLD version (no subsampling)\n",
    "    # z = (z_p @ w + b).flatten() # (N, 1) -> (N,)\n",
    "\n",
    "    # OLD version (no subsampling in plate)\n",
    "    # with numpyro.plate(\"data\", N):\n",
    "    with numpyro.plate(\n",
    "        \"data\",\n",
    "        X.shape[0],\n",
    "        subsample_size=subsample if subsample is not None else X.shape[0],\n",
    "    ) as idx:\n",
    "        # OR: introduce subsampling by index (y=> y_batch and X => x_batch)\n",
    "        x_batch = X[idx] if len(X.shape) > 1 else X\n",
    "        y_batch = y[idx] if y is not None and len(y.shape) > 0 else y\n",
    "        # OR: moved forward computation inside plate to allow subsampling\n",
    "        z = x_batch @ w0 + b0.flatten()\n",
    "        z_p = activation(z)\n",
    "        z = z_p @ w1 + b1\n",
    "        z_p = activation(z)\n",
    "        z = (z_p @ w2 + b2).flatten() # (N, 1) -> (N,)\n",
    "        if y_batch is not None:\n",
    "            assert z.shape == y_batch.shape, f\"Shapes (z,y): {(z.shape, y_batch.shape)}\"\n",
    "\n",
    "        y_loc = numpyro.deterministic(\"y_loc\", z)\n",
    "        numpyro.sample(\"y\", dist.Normal(y_loc, sigma), obs=y_batch)\n",
    "\n",
    "def model(x, y=None, depth=2, width=50, D_Y=1, subsample=None):\n",
    "    # OR: Introduced subsampling \n",
    "    prec = numpyro.sample(\"prec\", dist.Gamma(1.0, 0.1))\n",
    "    _sigma = jnp.sqrt(1 / prec)\n",
    "    BNN(x, y, depth=depth, width=width, D_Y=D_Y, sigma=_sigma, activation=nn.relu, subsample=subsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 3000/3000 [00:05<00:00, 500.60it/s, 1 steps of size 1.00e+00. acc. prob=0.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:  (3000, 5) (1, 3000, 5)\n",
      "NLL no batch:  -1674.3451\n",
      "NLL batch:  -1674.3451\n",
      "NLL log-sum-exp no batch:  -1674.3461\n",
      "NLL log-sum-exp batch:  -1674.3451\n",
      "NLL log-sum-exp batch, fixed:  -1674.3461\n"
     ]
    }
   ],
   "source": [
    "def simple_model(x, y=None):\n",
    "    a = numpyro.sample('a', numpyro.distributions.Normal(0, 1))\n",
    "    b = numpyro.sample('b', numpyro.distributions.Normal(0, 1))\n",
    "    sigma = numpyro.sample('sigma', numpyro.distributions.HalfNormal(1))\n",
    "    mu = a + b * x\n",
    "    numpyro.sample('y', numpyro.distributions.Normal(mu, sigma), obs=y)\n",
    "\n",
    "X = jnp.array([[1, 2, 3, 4, 5], [5,4,3,2,1]]).T\n",
    "y = X[:,0] * 2 + 1 + jax.random.normal(jax.random.PRNGKey(0), 5)\n",
    "\n",
    "nuts_kernel = numpyro.infer.NUTS(model)\n",
    "mcmc = numpyro.infer.MCMC(nuts_kernel, num_warmup=0, num_samples=3000)\n",
    "mcmc.run(jax.random.PRNGKey(0), X, y)\n",
    "post_draws_no_batch = mcmc.get_samples(group_by_chain=False)\n",
    "post_draws_batch = mcmc.get_samples(group_by_chain=True)\n",
    "\n",
    "post_draws_no_batch.pop(\"y_loc\", None)\n",
    "post_draws_batch.pop(\"y_loc\", None)\n",
    "\n",
    "nll_no_batch = numpyro.infer.log_likelihood(model, post_draws_no_batch, X, y=y, batch_ndims=1)\n",
    "nll_batch = numpyro.infer.log_likelihood(model, post_draws_batch, X, y=y, batch_ndims=2)\n",
    "\n",
    "print(\"Shapes: \", nll_no_batch[\"y\"].shape, nll_batch[\"y\"].shape)\n",
    "print(\"NLL no batch: \", nll_no_batch[\"y\"].mean())\n",
    "print(\"NLL batch: \", nll_batch[\"y\"].mean())\n",
    "\n",
    "print(\"NLL log-sum-exp no batch: \", (logsumexp(nll_no_batch[\"y\"], axis=0) - jnp.log(nll_no_batch[\"y\"].shape[0])).mean())\n",
    "print(\"NLL log-sum-exp batch: \", (logsumexp(nll_batch[\"y\"], axis=0) - jnp.log(nll_batch[\"y\"].shape[0])).mean())\n",
    "print(\"NLL log-sum-exp batch, fixed: \", (logsumexp(nll_batch[\"y\"], axis=(0,1)) - jnp.log(nll_batch[\"y\"].shape[1])).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-(logsumexp(lls, axis=0) - jnp.log(lls.shape[0])).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
